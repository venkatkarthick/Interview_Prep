# Facebook Messenger

## âœ… Facebook Messenger â€“ System Design Overview

---

## 1ï¸âƒ£ **What is Facebook Messenger?**

A real-time **instant messaging app** for **text communication** between users on **web & mobile**, supporting:

* One-on-one messaging
* Group chats
* Message history
* Push notifications

---

## 2ï¸âƒ£ **Core Requirements**

#### âœ… **Functional Requirements**

* One-on-one **text messaging**
* Track **user online/offline status**
* **Persist** message history (for sync across devices)

#### ğŸš€ **Extended Requirements**

* **Group chats** (multi-user messaging)
* **Push notifications** (offline alerts)

#### ğŸ›  **Non-Functional Requirements**

* **Real-time** messaging (low latency)
* **Strong consistency** (same chat across devices)
* **High availability** (optional trade-off with consistency)

---

## 3ï¸âƒ£ **Capacity Estimation (Back-of-the-Envelope)**

#### ğŸ“Š User Activity

* **500M daily active users**
* **40 messages/user/day** â†’ **20B messages/day**

#### ğŸ’¾ Storage Estimation

* 1 msg â‰ˆ **100 bytes**
* **Daily**: 20B \* 100Bytes = **2 TB/day**
* **5 years**: 2 TB/day \* 365 \* 5 = **3.6 PB**

(Excludes metadata, user info, replication, compression)

#### ğŸŒ Bandwidth Estimation

* **Incoming**: 2TB/day â‰ˆ **25 MB/s**
* **Outgoing**: Also **25 MB/s** (every sent message is delivered to someone)

---

### ğŸ” High-Level Numbers Summary

| Metric             | Value      |
| ------------------ | ---------- |
| Total messages/day | 20 Billion |
| Storage/day        | 2 TB       |
| Storage (5 years)  | 3.6 PB     |
| Incoming bandwidth | 25 MB/s    |
| Outgoing bandwidth | 25 MB/s    |

---

Hereâ€™s a **hydrated, clear, and interview-friendly version** of the **High-Level Design** for Facebook Messengerâ€™s messaging flow. This version keeps the technical details sharp while being easy to revise quickly:

---

## ğŸ—ï¸ 4. High-Level Design â€“ Facebook Messenger

### ğŸ¯ **Objective**

Design a system where users can **send and receive real-time messages**, with reliable delivery and message persistence.

---

### ğŸ§© Key Component: **Chat Server**

* Acts as the **central coordinator** for all message exchanges
* Responsibilities:

    * Accept messages from sender
    * **Route** them to receiver
    * **Store** them for history/syncing
    * Handle **acknowledgments** for reliability

---

### ğŸ” **End-to-End Messaging Workflow**

Letâ€™s break down the complete lifecycle of sending a message:

#### Step-by-step:

1. **User-A** types a message and sends it through an **open connection** (WebSocket or Long Poll) to the **Chat Server**

2. The **Chat Server**:

    * Accepts the message
    * Sends an immediate **ACK** to User-A (to confirm receipt)

3. The server:

    * **Stores** the message in the **database** (asynchronously, to avoid blocking)
    * **Forwards** the message to **User-B**, if they are online (via their open connection)

4. **User-B** receives the message and sends a **delivery acknowledgment** back to the server

5. The server then **notifies User-A** that the message has been successfully **delivered to User-B**

---

### ğŸ§  Reliability & UX Impact

| Event                         | Purpose                                           |
| ----------------------------- | ------------------------------------------------- |
| ACK to sender after receiving | Ensures sender doesnâ€™t resend due to network drop |
| Async DB storage              | Avoids delaying delivery due to disk I/O          |
| Final delivery ACK            | Helps update UI with "Delivered" tick âœ”ï¸          |

---

### ğŸ“ Interview Talking Point

You can say:

> The Chat Server plays a central role â€” it not only ensures messages are routed to the correct receiver but also handles persistence and delivery guarantees through a structured ACK flow. We optimize for **low latency** and **user feedback** without sacrificing **reliability**.

![LLD.png](LLD.png)

---

## ğŸ§© 5. Detailed Component Design â€“ Facebook Messenger

Weâ€™ll break it into three core areas:

1. **Message Handling**
2. **Message Storage**
3. **User Presence**

---

### ğŸ”¹ A. Message Handling

#### ğŸ¯ Goal:

Ensure **reliable, low-latency, real-time messaging** between users with **minimal delivery delay** and **accurate ordering**.

---

### ğŸ“¬ 1. Receiving and Delivering Messages

#### How do clients communicate with the server?

**Two main models:**

| Model      | Description                                                  | Pros                | Cons                                     |
| ---------- | ------------------------------------------------------------ | ------------------- | ---------------------------------------- |
| **Pull**   | Client periodically polls server for new messages            | Simple to implement | High latency, wasteful (empty responses) |
| âœ… **Push** | Client keeps an **open connection** (WebSocket/Long Polling) | Real-time delivery  | Slightly more complex infra              |

â¡ï¸ **Push model is preferred** for Messenger due to:

* **Instant delivery**
* **Low latency**
* Reduced resource wastage (no polling noise)

---

### ğŸ”— 2. Maintaining Open Connections

#### âœ… Technologies:

* **WebSocket** (full-duplex, low overhead)
* **HTTP Long Polling** (backup/older devices)

#### How does Long Polling work?

* Client sends a request to server, which **holds** it open
* Server replies only when new message is available
* Client **immediately reissues** a new request after response
* If timeout/disconnect happens, client reconnects

---

### ğŸ§  3. Connection Management

#### How does server know which user is connected and where?

* Use an **in-memory map/hash table**:

  ```text
  HashMap<UserID, ConnectionObject>
  ```
* Allows server to:

    * Quickly **look up** user connection
    * **Route** messages instantly

#### ğŸ” When user is offline:

* Server can:

    * âŒ Return "delivery failed" to sender
    * âœ… Ask sender to retry (client logic handles auto-resend)
    * âœ… Optionally **store and retry** when user reconnects

---

### ğŸ“ˆ 4. Scaling Chat Servers

#### Assumption:

* Each server supports **\~50K concurrent connections**
* For **500M active users**, we need:

  ```
  500,000,000 / 50,000 = 10,000 servers
  ```

#### ğŸ”„ Load Balancing:

* Use a **software load balancer** (or service registry) to:

    * Map `UserID â†’ Correct server`
    * Route connection and message delivery properly

---

### âœ‰ï¸ 5. Message Delivery Flow

Letâ€™s walk through the **exact steps** of delivering a message:

1. **Sender sends** a message via open connection to **Chat Server A**
2. Server A:

    * âœ… **Acknowledges** receipt to sender
    * ğŸ”„ **Looks up** which server holds receiver's connection (Chat Server B)
    * â¡ï¸ **Routes** message to Chat Server B
3. Chat Server B delivers to **receiver** (User-B)
4. User-B sends **delivery ACK**
5. Server notifies User-A that **delivery was successful**
6. In parallel, message is **stored asynchronously in DB**

---

### â³ 6. Message Ordering

#### â— Problem with relying only on timestamps:

Two users may send messages **almost simultaneously**, and server timestamps may vary slightly, e.g.:

```text
User-1 sends M1 â†’ Received at T1
User-2 sends M2 â†’ Received at T2 (T2 > T1)
```

If M2 reaches User-1 before M1 is seen by User-2, ordering becomes inconsistent across clients.

#### âœ… Solution: **Per-user sequence numbers**

* Each message has a **unique incremental sequence number per conversation per user**
* Guarantees **ordering is consistent for that user across all their devices**
* Doesnâ€™t force global order, which is unnecessary and hard to scale

| User   | View    | Order           |
| ------ | ------- | --------------- |
| User-1 | M1 â†’ M2 | Consistent      |
| User-2 | M2 â†’ M1 | Also consistent |

> Think of this as **local logical ordering**, not global physical ordering

---

### ğŸ“‹ Quick Recap: Message Handling Design Summary

| Feature          | Approach                             | Why                          |
| ---------------- | ------------------------------------ | ---------------------------- |
| Communication    | Push (WebSocket/Long Polling)        | Real-time, low latency       |
| Routing          | HashMap of UserID â†’ Connection       | Fast message delivery        |
| Scaling          | 10K chat servers + load balancer     | Handles 500M connections     |
| Message Delivery | ACK â†’ Route â†’ Deliver â†’ ACK          | Reliable experience          |
| Ordering         | Per-user sequence numbers            | Local consistency per client |
| Offline Handling | Auto-retry, fail response, or buffer | Flexible recovery            |

---
Here is a **hydrated, structured, and interview-friendly summary** of the **â€œStoring and Retrieving Messagesâ€** section of Facebook Messengerâ€™s system design.

This section focuses on **data durability, performance, and scale**, which are crucial for chat systems that handle **billions of messages daily**.

---

## ğŸ—„ï¸ 5b. Storing and Retrieving Messages â€“ Detailed Design

---

### ğŸ¯ Goal:

Design a **high-throughput**, **low-latency**, and **reliable** message persistence layer that:

* Handles **massive message volumes**
* Supports **sequential reads** for chat histories
* Ensures **message durability** and **eventual consistency**

---

### ğŸ” 1. Message Persistence Strategy

### âœ… Two Options to Store Incoming Messages:

| Option                      | Description                          | Trade-offs                                                |
| --------------------------- | ------------------------------------ | --------------------------------------------------------- |
| ğŸ§µ **Separate Thread**      | Spin off a new thread to write to DB | Simple, but can overwhelm thread pool under heavy load    |
| ğŸ“¨ **Asynchronous Request** | Use async/non-blocking I/O           | Preferred in scalable systems; better resource efficiency |

â¡ï¸ **Best practice:** Use an **async producer-consumer queue** (like Kafka or in-memory queue) and delegate DB writes to a **worker pool** or **background service**.

---

### âš™ï¸ 2. Design Considerations

To support the async DB write pipeline:

### ğŸ§© Key Factors to Handle:

1. **Connection Pooling** â€“ Efficient use of DB connections to avoid exhaustion
2. **Retry Mechanism** â€“ In case of transient failures (e.g., network blips)
3. **Dead-letter Logging** â€“ Store logs of failed write attempts (after multiple retries)
4. **Replay Mechanism** â€“ Background job periodically retries failed logs

> ğŸ’¡ This design ensures **durability and recoverability** without blocking the message delivery path.

---

### ğŸ§¬ 3. Database Selection

### âŒ What **not to use**:

* **Traditional RDBMS (MySQL)**:

    * Good for relational queries, but not designed for high write throughput or range scans on massive unstructured data
* **Document stores (MongoDB)**:

    * Doesnâ€™t scale well for workloads with **frequent, small-sized writes**

---

### âœ… Recommended: **Wide-Column Store (e.g., HBase)**

#### ğŸ” Why HBase?

| Feature                    | Benefit                                                               |
| -------------------------- | --------------------------------------------------------------------- |
| **High write throughput**  | Excellent for chatâ€™s firehose of small messages                       |
| **Range scans**            | Ideal for reading message history in order                            |
| **Columnar model**         | Supports storing multiple versions of a message per key               |
| **Runs on HDFS**           | Durable, scalable, and distributed                                    |
| **Modeled after BigTable** | Proven for real-time messaging at scale (e.g., Gmail, WhatsApp infra) |

---

### ğŸ“¦ HBase Message Schema (Example)

| Row Key (UserID\_ConversationID) | Column Family: `msg`              | Columns: timestamp/message |
| -------------------------------- | --------------------------------- | -------------------------- |
| `user1_chat2`                    | `msg:2024-07-15T12:01:00Z` â†’ "Hi" | â€¦                          |

> âœ… This allows **range queries**, e.g., *"Give me the last 50 messages between user1 and chat2."*

---

### ğŸ“² 4. Message Retrieval (Client-Side)

### ğŸ§¾ How should clients fetch chat history?

**Use Pagination:**

* Clients **request a window** of messages, not all at once
* Pagination is usually based on:

    * `timestamp` or
    * `sequence number` or
    * `message ID range`

#### Why?

| Client Type | Page Size   | Rationale                       |
| ----------- | ----------- | ------------------------------- |
| ğŸ“± Mobile   | 20â€“30 msgs  | Small screen, limited bandwidth |
| ğŸ’» Desktop  | 50â€“100 msgs | Larger screen, faster network   |

â¡ï¸ Load messages **lazily**, e.g., when the user scrolls up (infinite scroll UX)

---

### ğŸ› ï¸ Design Pattern for Efficient Fetching:

* **Backend API** should expose endpoints like:

  ```
  GET /conversations/:id/messages?before=timestamp&pageSize=50
  ```
* Backend fetches from HBase using **range scans on row keys**
* Applies **rate limiting** and **authorization** per user

---

### ğŸš€ Recap: Message Storage Design Summary

| Category         | Choice                                         | Why                                         |
| ---------------- | ---------------------------------------------- | ------------------------------------------- |
| Writing Strategy | Async + Retry + Logging                        | Durable and efficient                       |
| Storage DB       | âœ… HBase (wide-column)                          | High write throughput, supports range reads |
| Failure Handling | Retry queue + dead-letter log + replayer       | Ensures eventual durability                 |
| Read Pattern     | Paginated fetch, lazy loading                  | Scales to billions of messages              |
| Data Model       | Row = Conversation, Columns = Timestamped Msgs | Enables fast sequential reads               |

---


## 5C - ğŸ”µ Component: User Presence Management (Online/Offline Status)

### ğŸ¯ **Goal**

Efficiently **track and communicate** user presence (online/offline) to **relevant users** without overwhelming the system.

---

### ğŸ’¡ Why is it challenging?

* **500M+ active users** = Massive scale
* If we **broadcast every status change** (e.g., every "User X is now online") to all friends instantly â†’
  âŒ Unsustainable load on server & network

---

### ğŸ§  Core Insight

âœ… Since each online user maintains an **open connection** (WebSocket or Long Poll), the server inherently knows who's online.

So, the challenge isn't **tracking**, but **efficiently sharing** this info with others.

---

### âš™ï¸ Optimized Presence Handling â€“ Key Techniques

#### 1ï¸âƒ£ **App Launch: Initial Bulk Pull**

* When user opens the app:

    * Client sends request:
      *"Give me the current status of all my friends."*
    * Server responds with **snapshot of statuses**
    * âœ… Reduces real-time tracking load

#### 2ï¸âƒ£ **Message-to-Offline Detection**

* If **User A** sends a message to **User B**, and **B is offline**:

    * Server:

        * âŒ Fails delivery
        * âœ… Notifies User A
    * Client updates **status of User B = offline**
    * âœ… Saves from broadcasting offline status constantly

#### 3ï¸âƒ£ **Online Status Broadcast with Delay**

* When a user connects:

    * Server **delays broadcast** (e.g., 5â€“10 seconds)
    * Why? Users may come online for a brief moment (network drops, refreshes)
    * âœ… Reduces â€œstatus flappingâ€

#### 4ï¸âƒ£ **Viewport-Aware Status Pulling**

* Clients often display a **subset of friends** (e.g., top 5 in viewport)
* Client periodically pulls **only the visible users' statuses**

    * âœ… Reduces load
    * â— Some staleness is acceptable (e.g., if friend just went offline)

#### 5ï¸âƒ£ **Chat Initiation Check**

* When starting a **new conversation**, client:

    * Pulls status of target user **on-demand**
    * âœ… Ensures up-to-date info without periodic checks

---

### ğŸ“ Summary Table

| Optimization                       | When it happens               | What it saves                       |
| ---------------------------------- | ----------------------------- | ----------------------------------- |
| Initial status pull                | On app start                  | Avoids tracking all friends live    |
| Message delivery failure detection | On sending to an offline user | No need for pre-checks              |
| Delayed status broadcast           | On new connection             | Reduces false-positive flaps        |
| Viewport-based pulling             | When users are in view (UI)   | Avoids unnecessary full-list checks |
| On-demand fetch on new chat start  | When opening new DM           | Saves tracking everyone always      |

---

### ğŸ§  Interview Tip

If asked *â€œhow would you reduce load while maintaining presence accuracy?â€*, explain:

> We **donâ€™t need real-time accuracy for all friends**. By combining **delayed updates**, **on-demand checks**, and **lazy pulling**, we strike a balance between **user experience** and **system scalability**.

---

## ğŸ§© 6. Data Partitioning â€“ Facebook Messenger

### ğŸ¯ Goal:

Efficiently store and retrieve **petabytes of chat data** across distributed storage systems, while enabling **fast read access** to chat history.

---

## ğŸ“¦ Storage Scale

* Expected storage: **3.6 Petabytes (PB)** for 5 years
* If one DB shard = **4TB**, then:

  ```
  3.6PB / 4TB â‰ˆ 900 shards
  ```

---

## ğŸ”‘ Partitioning Strategy

### âœ… **Primary: Hash-based Partitioning by `UserID`**

#### Why?

| Reason                     | Explanation                                                  |
| -------------------------- | ------------------------------------------------------------ |
| âœ… **Locality of data**     | All of a user's messages are on the same shard               |
| âœ… **Fast reads**           | Chat history queries are mostly per-user or per-conversation |
| âœ… **Uniform distribution** | Hashing avoids hotspots and balances load across shards      |
| âœ… **Sharding scalability** | Easy to scale with new users                                 |

#### Shard Calculation:

```
shardNumber = hash(UserID) % 1000
```

* We predefine **1000 logical shards**
* Even if only a few physical servers exist initially, we can map multiple logical shards to each server

---

### ğŸ” Logical vs Physical Partitioning

#### ğŸ”§ Initial Setup:

* Start with **few physical DB servers**
* Each server stores **multiple logical partitions (shards)**

#### ğŸ› ï¸ As system grows:

* Add more physical servers
* **Reassign logical shards** to new physical hosts using **consistent hashing** or shard rebalancing

> ğŸ’¡ Hashing abstracts away the physical server from the application logic â€” making the system **scalable and transparent**.

---

### âŒ Alternative: Partitioning by `MessageID`

| Why itâ€™s a bad idea                                   |
| ----------------------------------------------------- |
| âŒ Spreads a single userâ€™s messages across many shards |
| âŒ Range queries (chat history) become **very slow**   |
| âŒ Adds complexity in fetching ordered messages        |

> âš ï¸ Use-case violation: Messaging apps **almost always query by conversation/user**, not by individual message ID.

---

## ğŸ“Œ Summary Table

| Aspect               | Value                                                      |
| -------------------- | ---------------------------------------------------------- |
| Total Storage Needed | 3.6 PB for 5 years                                         |
| Shard Size           | 4 TB                                                       |
| Total Shards         | \~900 (rounded to 1000 for simplicity)                     |
| Partition Key        | `UserID` (hash-based)                                      |
| Shard Mapping        | `hash(UserID) % 1000`                                      |
| Initial Deployment   | Fewer physical servers, multiple logical shards per server |
| Scale-Up Strategy    | Add physical servers, remap shards                         |
| Avoided Strategy     | Partitioning by `MessageID` (due to read inefficiency)     |

---

## ğŸ—£ï¸ Interview Talking Point

You can say:

> To ensure scalability and fast retrieval of chat history, we use a **hash-based partitioning scheme on `UserID`**. This way, all messages for a user go to the same shard, allowing efficient range queries. We separate **logical shards** from **physical servers** so that we can scale storage transparently by reassigning shards when needed.

## ğŸ”„ 7. Caching â€“ Speeding Up Chat Access

### ğŸ¯ Goal:

Reduce latency and database load for frequent or recent conversations.

### ğŸ’¡ Strategy:

* **Cache the latest N messages** (e.g., last 15) from the **last M conversations** (e.g., last 5) shown in the userâ€™s viewport.
* This supports instant access to recent messages when opening Messenger or scrolling chat lists.

### ğŸ“ Design Choice:

* Since a userâ€™s messages are all stored on one shard, **cache for a user can also reside on a single server**, simplifying cache consistency and access.

### âœ… Benefits:

| Feature              | Why It Helps                          |
| -------------------- | ------------------------------------- |
| Caching last 5 chats | 80%+ of usage comes from recent chats |
| Avoid DB hits        | Reduces pressure on the main database |
| Co-located cache     | Local to the DB shard = low latency   |

---

## âš–ï¸ 8. Load Balancing â€“ Routing User Traffic

### ğŸ§© Components to Load Balance:

1. **Chat Servers** (handling live WebSocket/TCP connections)
2. **Cache Servers**

### ğŸ§  Strategy:

* Use **consistent hashing on `UserID`** to route the request to the appropriate chat server.
* This keeps **user-session affinity**, ensuring all messages from/to a user land on the **same chat server**.
* For cache servers, similarly **map `UserID` to a cache node**, reducing lookups and cache misses.

### ğŸ” Chat Server Affinity:

```
chatServer = hash(UserID) % totalServers
```

> This ensures that the same user always connects to the same backend unless a server crashes.

---

## ğŸ” 9. Fault Tolerance & Replication â€“ Ensuring Reliability

### ğŸš¨ Chat Server Failure

#### ğŸ”´ Problem:

* Chat servers maintain **active TCP/WebSocket connections**.
* **TCP connections cannot be failed over** to another server easily.

#### âœ… Solution:

* Let the **client handle reconnection** logic.
* On failure, the client will automatically **reconnect to another healthy server** (possibly assigned by a load balancer).

### ğŸ§  Redundancy for Message Data

| Question                                        | Design Decision                                    |
| ----------------------------------------------- | -------------------------------------------------- |
| Can we have only one copy of a user's messages? | âŒ No, that risks permanent data loss               |
| Solution?                                       | âœ… Store **multiple copies** (e.g., 3 replicas)     |
| Advanced Option                                 | Reed-Solomon encoding for **efficient redundancy** |

#### âœ… Implementation Options:

* Use **replication across HBase regions**
* Or apply **erasure coding (Reed-Solomon)** for storage-efficient backups

> ğŸ” All writes should go to the **primary + replicas** to ensure consistency and recoverability.

---

## ğŸ—£ï¸ Interview Soundbites

You can say:

> For performance, we cache recent messages of active conversations in memory, colocated with the userâ€™s DB shard. For reliability, we replicate message data across nodes and let clients reconnect to chat servers after failure, since TCP failover is hard. Load balancing is done via consistent hashing on `UserID` to maintain session stickiness.

---

![img.png](HLD.png)

## ğŸ”§ 10. Extended Requirements â€“ Advanced Use Cases

---

### ğŸ«‚ a. Group Chat Support

#### ğŸ§  Core Idea:

* Use a **GroupChat object** identified by a unique `GroupChatID`.
* Each group chat maintains a **member list** of participants and lives on a **specific chat server**.

#### ğŸ› ï¸ Architecture:

* When a message is sent to a group:

    * Load balancer uses `GroupChatID` to route the message to the correct chat server.
    * That server **iterates over all group members**, finds their respective chat connections (via UserID â†’ Chat Server mapping), and sends the message.

#### ğŸ§© Storage Strategy:

* Group chat metadata is stored in a **separate database table**.
* Partitioning is done by `GroupChatID` for fast access and scalability.

#### âœ… Design Benefits:

| Feature              | Why It Helps                            |
| -------------------- | --------------------------------------- |
| Group-based routing  | Avoids centralized bottlenecks          |
| Distributed delivery | Parallel push to each user's connection |
| Partitioned storage  | Efficient lookup and growth             |

> ğŸ—£ï¸ In interviews, mention that each chat server only needs to know the users it is responsible for and forward group messages to relevant servers.

---

### ğŸ“² b. Push Notifications for Offline Users

#### ğŸš« Problem:

* In the original design, **messages to offline users fail**, which is not ideal in real-world usage.

#### âœ… Solution: Add Push Notification Support

#### ğŸ”„ Workflow:

1. **Client Opt-In**:

    * Devices register for push notifications via manufacturer services (e.g., **APNs**, **FCM**).

2. **Notification Server**:

    * A new internal component is introduced.
    * Responsible for:

        * Detecting when a recipient is offline
        * Formatting the notification payload
        * Forwarding it to the manufacturerâ€™s push service

3. **Manufacturer Service**:

    * Handles the final **push to the userâ€™s device** using tokens provided by the client app.

#### ğŸ”’ Persistence Note:

* You may still **persist the actual message in DB**, and just push a notification like:

  > â€œYou have a new message from Alexâ€
* This avoids sending sensitive content via push, respects security policies.

---

### ğŸ§  Design Considerations:

| Concern                         | Approach                                   |
| ------------------------------- | ------------------------------------------ |
| Device-specific APIs            | Integrate with APNs, FCM, etc.             |
| Offline awareness               | Server should maintain user presence       |
| Failure to deliver push         | Retry logic, TTL, or fallback to SMS/email |
| Notification server scalability | Stateless, horizontally scalable service   |

---

## ğŸ’¬ Interview Soundbites

You can say:

> For group chats, we route messages based on `GroupChatID` and deliver to all active participants by checking their connected chat server. To support offline messaging, we added a Notification Server that pushes messages via FCM or APNs when the recipient is offline. Group chat metadata and messages are partitioned by `GroupChatID` for efficient storage and lookup.

